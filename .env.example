# Copy this file to .env and fill in your API keys
# DO NOT commit .env to version control - it contains secrets!

# =============================================================================
# OPENAI CONFIGURATION
# =============================================================================
# Supports both openai.com and Azure OpenAI with separate resources for chat and embeddings
# For Azure OpenAI: Get these from Azure portal (https://portal.azure.com)
# For openai.com: Get key from https://platform.openai.com/api-keys

# ===== CHAT/LLM Configuration =====
# Your OpenAI API key (for chat/LLM)
OPENAI_API_KEY="your-openai-api-key-here"

# API Endpoint for chat/LLM (REQUIRED for Azure OpenAI, leave empty for openai.com)
# Azure format: https://your-resource.openai.azure.com/openai/v1/
# OpenAI format: (leave empty - will use default)
OPENAI_API_ENDPOINT="https://your-resource.openai.azure.com/openai/v1/"

# Chat model ID
# - For Azure: deployment name (e.g., gpt-4o, gpt-5-mini)
# - For openai.com: model ID (e.g., gpt-4o, gpt-4-turbo, gpt-3.5-turbo)
OPENAI_CHAT_MODEL="gpt-4o"

# ===== EMBEDDING Configuration (can be different resource) =====
# API Endpoint for embeddings (optional - can be different resource in Azure)
# Leave empty to use OPENAI_API_ENDPOINT for embeddings too
# Azure format: https://eastus.api.cognitive.microsoft.com/ or deployment-specific endpoint
OPENAI_EMBEDDING_ENDPOINT="https://eastus.api.cognitive.microsoft.com/"

# Embedding model ID
# - For Azure: deployment name (e.g., text-embedding-3-small)
# - For openai.com: model ID (e.g., text-embedding-3-small, text-embedding-3-large)
OPENAI_EMBEDDING_MODEL="text-embedding-3-small"

# API key for embeddings (optional - leave empty to use OPENAI_API_KEY)
# Only set if embeddings use a different Azure resource with different credentials
# OPENAI_EMBEDDING_API_KEY="your-embedding-api-key-if-different"

# =============================================================================
# NEO4J CONFIGURATION
# =============================================================================
# Connection to Neo4j database (running via docker-compose)

# Neo4j bolt connection URI
NEO4J_URI="bolt://localhost:7687"

# Neo4j username (default from docker-compose.yml)
NEO4J_USER="neo4j"

# Neo4j password (change in production!)
NEO4J_PASSWORD="password"

# =============================================================================
# TAVILY SEARCH CONFIGURATION
# =============================================================================
# Get this from Tavily (https://tavily.com)

# Tavily API key for web search functionality
TAVILY_API_KEY="your-tavily-api-key-here"

# =============================================================================
# AGENT CONFIGURATION
# =============================================================================

# Display name for the agent
AGENT_NAME="Knowledge Graph Agent"

# Number of recent conversation turns to keep in LLM context window
# (Older turns are still available in knowledge graph)
CONVERSATION_HISTORY_LIMIT=10

# =============================================================================
# SETUP INSTRUCTIONS
# =============================================================================
#
# 1. Copy this file: cp .env.example .env
#
# 2. Get your API keys:
#    - OpenAI: https://platform.openai.com/api-keys
#    - Tavily: https://tavily.com/dashboard
#
# 3. Fill in the keys above
#
# 4. Start Neo4j: docker-compose up -d neo4j
#
# 5. Test setup: python main.py
#
# =============================================================================
